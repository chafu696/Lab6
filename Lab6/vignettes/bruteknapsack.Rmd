---
title: "bruteknapsack"
author: "Chao Fu & Michael Bailey" 
data: "r Sys.Date()"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bruteknapsack}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(bruteknapsack)
```
# Description
There are three functions to solve the knapsack problem, each function implements
a algorithm.

# Functions

## brute_force_knapsack()
1. The function implements the algorithm with complexity O(2^n). 
1. The return is an list with the biggest value and the items.

## dynamic_knapsack()
1. The function implements the dynamic programming algorithm. 
1. The return is an list with the biggest value and the items.

## greedy_knapsack()
1. The function implements the greedy algorithm. 
1. The return is an list with the biggest value and the items.

# Questions
** How much time does it takes to run the brute force algorithm for n = 16 objects? **
It takes ~0.24 seconds to run with 16 objects and W=3500, roughly 2^4^ times as long as with 12 objects. Profiling with high W shows that about half the time is spent on generating the combinations in the for loop and half is spent on the two sums and comparisons. With a lower W, more combinations are discarded after the first check so it runs faster and the combination generation takes a greater proportion of the time. Other methods were tried but were found slower. Adding combinations that went over the weight limit to a "banned" list to attempt to get the algorithm to ignore eg (1,2,3,4) if (1,2,3) was over-weight was not helpful with our implementation.

** How much time does it takes to run the dynamic algorithm for n = 500 objects? **
It takes 37 seconds to run the algorithm for 500 objects.

** How much time does it takes to run the greedy algorithm for n = 1000000 objects? **
It takes 0.03 seconds to run the greedy algorithm for 1000000 objects. During profiling with 20000000 objects, one of the changes attempted was using dplyr::arrange() instead of order(), which with our implementation required adding an ID column. This was neutral in terms of time gained or lost, so it suggests dplyr::arrange() may be faster if the extra column was not required.

** What performance gain could you get by trying to improving your code? **
From the initial implementation of the brute force algorithm, this version is 300% faster, so poorly implemented code can cause issues especially during computationally heavy things such as a brute force algorithm. Minimising the memory usage can be useful in theory as well, though not especially important for our applications. Well written code can speed up functions in the best case by a lot, by skipping calculations that may not be needed. 

# Usage

## Install

```{r, eval=FALSE}
devtools::install_github("chafu696/Lab6")

```

## Example

```{r, eval=FALSE}
set.seed(42)
n <- 2000
knapsack_objects <- data.frame(w = sample(1:4000, size = n , replace = TRUE),
v = runif(n = n, 0 , 10000))
bfk <- brute_force_knapsack(x = knapsack_objects[1:8, ], W = 3500)
```
```{r, eval=FALSE}
set.seed(42)
n <- 2000
knapsack_objects <- data.frame(w = sample(1:4000, size = n , replace = TRUE),
v = runif(n = n, 0 , 10000))
bfk <- dynamic_knapsack(x = knapsack_objects[1:8, ], W = 3500)
```
```{r, eval=FALSE}
set.seed(42)
n <- 2000
knapsack_objects <- data.frame(w = sample(1:4000, size = n , replace = TRUE),
v = runif(n = n, 0 , 10000))
bfk <- greedy_knapsack(x = knapsack_objects[1:800, ], W = 3500)
```
